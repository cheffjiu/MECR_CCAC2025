## 一、赛题解读

MECR\_CCAC2025 赛题旨在对双人对话中的情感变化位置进行定位，并生成人类认知水平的“原因说明”(rationale)。具体来说，给定一段多模态对话（文本＋对应短视频），以及标注的情感变化起止点，模型需要输出由四部分组成的文本串：

```
{textual stimulus}；{visual stimulus}。{appraisal}。{response}。
```

* **textual stimulus**：对话中触发情感变化的文本内容。
* **visual stimulus**：相应时间点的视频帧中视觉触发因素。
* **appraisal**：主观认知评估，对触发因素的评价。
* **response**：最终的情感反应或行为。

### 赛题难点

1. **多模态信息融合**：文本与视觉信息形式差异巨大，需要对齐时序并跨模态交互。
2. **情感变化定位**：模型需准确判断情感变化的起止 utterance，并提取相应截帧。
3. **因果解释生成**：生成流畅、连贯且符合人类思维的因果说明，要求模型具备一定的常识和逻辑推理能力。

## 二、数据概览与数据流

### 1. 赛题提供的数据

* **train.json / val.json**：每条样本包含对话若干条 utterance、说话人标签、时间戳、emotion 标注以及情感变化的起止 utterance ID 和人类标注的 rationale 文本。
* **memoconv\_convs/**：与每条样本对应的视频片段文件夹，存储若干关键帧。
* **readme.pdf / README.md**：数据字段说明与评测流程文档。

### 2. 数据流概述

1. **原始数据读取**：从 JSON 中加载对话文本与情感变化标签，并定位对应视频帧。
2. **多模态特征提取**：文本通过 BERT/RoBERTa 编码为向量；视频通过 CLIP/Faster-RCNN 提取视觉特征。
3. **检索示例提取**：利用 FAISS 向量库检索文本刺激、视觉刺激与完整 rationale 示例。
4. **情感演化图构建**：将 utterance 与变化区间建图，通过图神经网络得到变化核心向量。
5. **Prompt 拼接与 LLM 解码**：将上下文 embedding、检索示例和图向量按照统一因果 Prompt 格式拼接，输入 LoRA 微调的多模态大模型生成最终 rationale。

### 3. 解决后得到的数据

* **features/text/、features/vision/**：每个样本的文本与视觉 embedding 文件（.npy 格式）。
* **indices/faiss\_\*.idx**：三套 FAISS 索引库，用于实时检索示例。
* **graphs/{id}.pt**：PyG 序列化的对话图数据，包含节点属性和边列表。
* **checkpoints/**：图编码器与 LoRA 微调模型权重。
* **predictions.json**：最终生成的 rationale 输出文件，可直接用于评测。

## 三、解决方案概览

本方案融合了统一因果 Prompt、检索增强、图神经情感演化与 LoRA 微调多模态大模型四大核心技术。整个流程分为四个阶段：

1. **数据准备与检索库构建**
2. **情感演化图构建与编码**
3. **Prompt 组装与 LoRA 微调训练**
4. **推理与部署**

每个阶段都明确了输入/输出数据格式、所用模型与技术，以及操作目的。

## 四、实施步骤

### 阶段 1：数据准备与检索库构建

**步骤 1.1 解析原始数据**
编写脚本 `scripts/prepare_data.py` 将 train.json 和 val.json 中的样本整理为统一格式，包括 utterance 列表、说话人标签、frame 路径以及 rationale 文本。输出 JSON 列表，便于后续批量处理。

* **输入**：train.json, val.json, memoconv\_convs/
* **输出**：`data/samples.json`，包含所有样本的基本信息。
* **为什么**：统一数据格式，简化后续特征提取与加载逻辑。

**步骤 1.2 文本与视觉特征提取**
使用 BERT-Base 将每条 utterance 文本编码为 768 维向量，保存在 `features/text/{id}.npy`；使用 CLIP-ViT 或 Faster‑RCNN 对情感变化前后各 3 帧进行视觉特征提取，保存在 `features/vision/{id}.npy`。

* **输入**：`data/samples.json`；对应媒体帧文件。
* **输出**：`features/text/`, `features/vision/` 下的 .npy 文件。
* **为什么**：将多模态原始数据转换为定长向量，便于后续检索与快速加载。

**步骤 1.3 检索库构建**
从训练集的 rationale 文本中拆分三类示例：textual stimulus、visual stimulus、完整 rationale。分别用相同编码器向量化并存入三个 FAISS 索引(`faiss_text.idx`, `faiss_vision.idx`, `faiss_rationale.idx`)，支持 Top-K 检索。

* **输入**：`features/text/`, `features/vision/`, `data/samples.json` 中 rationale 文本。
* **输出**：`indices/faiss_*.idx`。
* **为什么**：检索示例可为生成提供高质量范例，引导模型生成符合人类标注风格的解释。

### 阶段 2：情感演化图构建与编码

**步骤 2.1 图数据结构生成**
脚本 `scripts/build_graphs.py` 读取每个样本的文本与视觉向量，将每条 utterance 拼接为节点属性，新增表示变化区间的超级节点 S，并将其与变化前后 utterance 相连，同时增加对话顺序边和同说话人全连边。保存为 PyG Data 对象。

* **输入**：`features/text/{id}.npy`, `features/vision/{id}.npy`
* **输出**：`graphs/{id}.pt`。
* **为什么**：图结构显式表示对话的时序与说话人互动，帮助模型捕捉情感如何演化。

**步骤 2.2 图编码器训练（可选）**
使用 Temporal Graph Network (TGN) 对所有训练样本图进行预训练。以对比损失使超级节点 S 的输出向量 `h_change` 与真实 rationale 文本 embedding 对齐，为下游提供更 semantically rich 的情感变化表示。

* **输入**：`graphs/{id}.pt`
* **输出**：图编码器权重 `checkpoints/graph_encoder.ckpt`。
* **为什么**：预训练的图编码器能够学习对话图中情感演化的模式，生成更精准的变化核心向量。

### 阶段 3：Prompt 组装与 LoRA 微调训练

**步骤 3.1 环境搭建与模型准备**
安装 `transformers`, `peft`, `accelerate` 等库，加载预训练多模态 LLM（GPT-4V 或 Flamingo）。

* **输出**：基础模型与脚本环境。
* **为什么**：准备好可微调的多模态大模型。

**步骤 3.2 DataLoader 与特征集成**
在 `train_lora.py` 中，针对每个 batch：

1. 加载样本 ID 并读取对应的文本/视觉向量。
2. 在三个 FAISS 索引中分别检索 Top-3 示例，加载示例原文。
3. 读取对应 `graphs/{id}.pt`，通过预训练图编码器提取 `h_change`。

* **输入**：样本列表、features、indices、graphs、graph\_encoder.ckpt。
* **输出**：模型每次迭代的完整 Prompt 组件。
* **为什么**：整合多模态与检索示例，以及结构化变化向量，构建最丰富的 Prompt 上下文。

**步骤 3.3 Prompt 拼接与 LoRA 配置**
将 Context、ChangeVec、RetrievedText、RetrievedVis、RetrievedRationale 按统一模板拼接为完整 Prompt；在模型指定层插入 LoRA Adapter，仅训练 Adapter 参数。

* **为什么**：Prompt 统一因果模板保证模型同时关注情感位置与原因，并以范例示例增强；LoRA 降低微调成本。

**步骤 3.4 损失函数与训练**
定义总损失为交叉熵生成损失与图-文本对齐对比损失的加权和。采用 AdamW 优化，学习率 warmup + cosine decay。训练若干 epoch 并在验证集上早停。

* **为什么**：联合优化生成质量与情感变化向量对齐，提升模型一致性与准确度。

### 阶段 4：推理与部署

**步骤 4.1 推理脚本实现**
`infer.py` 按训练流程读取特征、检索示例、图编码，拼接 Prompt 并调用微调后的 LoRA 模型生成 rationale。

* **输出**：`predictions.json`，每条样本对应最终生成文本。
* **为什么**：保持与训练一致的流水线，保证推理结果稳定。

**步骤 4.2 评测与优化**
使用官方评测脚本计算 METEOR 和 BERTScore 排名和，验证效果；根据误差分析可微调检索参数或 Prompt 设计。

**步骤 4.3 服务化部署**
将模型和流水线封装为 RESTful 服务，可接收对话文本和视频片段路径，返回生成的 rationale，为实际应用场景提供接口。

---

以上详尽阐述了从赛题解读、数据流、数据产物，到融合方案的四大实施阶段。每一步都明确了输入/输出、所用模型与技术、处理原因与目标，确保团队能按图索骥，快速落地并取得竞赛佳绩。

```
data/MECR_CCAC2025/
├── processed/  
│   ├── processed_samples.json      # 样本元数据（你的 JSON 列表）
│   └── features/                   # 存放每条样本的特征文件
│       ├── anjia_sample1.pt
│       ├── anjia_sample2.pt
│       └── anjia_sample44.pt
每个 *.pt 文件是一个 PyTorch 字典（dict），Key 包括：

"text_feats"：Tensor[N, D_t]，N = utterance 数，D_t ≈ 768

"vis_feats"：Tensor[N, D_v]，D_v ≈ 512；对于没有关键帧的 utt，对应行全零

"vis_mask"：Tensor[N]，二值掩码（1=有视觉特征，0=无视觉特征）

（可选）"speaker_ids"：Tensor[N]，A/B 映射后的整型 ID

（可选）"emo_ids"：Tensor[N, E]，情感标签 one‑hot 或 embedding 后的索引

、、、

UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause
https://arxiv.org/abs/2404.00403

Generative Emotion Cause Explanation in Multimodal Conversations
https://arxiv.org/abs/2411.02430

Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models
https://arxiv.org/abs/2504.07521