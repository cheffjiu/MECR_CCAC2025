éå¸¸å¥½ï¼ä½ é€‰æ‹©çš„ä¸¤æ¡è·¯çº¿éƒ½æ˜¯å½“å‰å¤šæ¨¡æ€è¯­è¨€å»ºæ¨¡ä¸­æœ€æœ‰æ•ˆçš„æ–¹æ¡ˆä¹‹ä¸€ã€‚ä¸‹é¢æˆ‘å°†ä¸ºä½ **åˆ†åˆ«æ˜ç¡®è¿™ä¸¤æ¡è·¯å¾„çš„ç»“æ„è®¾è®¡ä¸å®æ–½è®¡åˆ’**ï¼Œå¹¶è¯´æ˜å®ƒä»¬å¦‚ä½•ååŒå·¥ä½œã€‚

---

## âœ… è·¯çº¿ 1ï¼šå°† `h_change` æ¥å…¥è¯­è¨€æ¨¡å‹å†…éƒ¨ï¼ˆCross-Attentionï¼‰

### ğŸ¯ ç›®æ ‡

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLaMAã€Qwenã€ChatGLM ç­‰ï¼‰ä¸­æ’å…¥æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½åœ¨ç”Ÿæˆ rationale æ—¶å‚è€ƒå›¾ç¥ç»ç½‘ç»œæå–çš„æƒ…æ„Ÿå˜åŒ–å‘é‡ $h_{\text{change}} \in \mathbb{R}^D$ã€‚

---

### ğŸ“Œ å®ç°æ–¹æ¡ˆï¼šæ’å…¥ Cross-Attention æ¨¡å—ï¼ˆEncoder-Decoder é£æ ¼ï¼‰

#### æ¨¡å—ç»“æ„ï¼š

```
Embedding â†’ Transformer Layer Ã— N
               â†“
           Cross-Attn(h_change)
               â†“
         â†’ ç”Ÿæˆ rationale
```

#### ä½ åªéœ€ï¼š

* å†»ç»“ä¸»å¹² transformerï¼›
* æ’å…¥ä¸€ä¸ªè½»é‡æ¨¡å—ï¼ˆå¦‚ LoRA å±‚æˆ– cross-attn å±‚ï¼‰ï¼›
* ç”¨ `h_change` ä½œä¸º KVï¼Œæ–‡æœ¬ embedding ä½œä¸º Qã€‚

#### âœ… æ’ä»¶ä»£ç ï¼ˆè‰å›¾ï¼‰ï¼š

```python
class HChangeCrossAttention(nn.Module):
    def __init__(self, d_model, num_heads=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads=num_heads, batch_first=True)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, hidden_states, h_change):  # h_change: [B, D]
        h_change = h_change.unsqueeze(1)         # [B, 1, D]
        attn_out, _ = self.attn(hidden_states, h_change, h_change)
        return self.norm(hidden_states + attn_out)
```

#### âœ… é›†æˆæ–¹å¼ï¼š

* åœ¨ HuggingFace `transformers` æ¨¡å‹ç»“æ„ä¸­æ’å…¥æ­¤å±‚ï¼›
* æˆ–æ›´ç®€å•åœ°ï¼Œåœ¨ **LoRA å¾®è°ƒä¸­ï¼Œæ¥å…¥æ­¤æ¨¡å—ï¼Œä½œä¸ºç‹¬ç«‹è·¯å¾„è®­ç»ƒ**ï¼›
* å¯åªæ’å…¥ä¸­é—´å±‚ 6/12/18 å±‚ä¹‹ä¸€ï¼ˆå¦‚ä»…å¯¹ Decoder Block 9 åŠ ï¼‰

---

## âœ… è·¯çº¿ 2ï¼šç”¨ `h_change` åšæ£€ç´¢å¢å¼º Promptï¼ˆè½¯æŒ‡å¯¼ï¼‰

### ğŸ¯ ç›®æ ‡

ç”¨ `h_change` å‘é‡åœ¨è®­ç»ƒé›†æˆ–å¤–éƒ¨åº“ä¸­æ£€ç´¢ä¸ä¹‹è¯­ä¹‰ç›¸ä¼¼çš„æ ·æœ¬ï¼ˆç›¸ä¼¼æƒ…æ„Ÿå˜åŒ–ï¼‰ï¼Œå†å°†å…¶ rationale æ‹¼å…¥ promptï¼Œå¼•å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´åˆç†è§£é‡Šã€‚

---

### ğŸ“Œ å®ç°æ­¥éª¤

#### â‘  å»ºç«‹æ£€ç´¢åº“

* æŠŠè®­ç»ƒé›†ä¸­æ‰€æœ‰æ ·æœ¬çš„ `h_change` ä¿å­˜ä¸º `.pt` æ–‡ä»¶ï¼›
* é¢„å…ˆç¼–ç ä¸ºä¸€ä¸ª FAISS indexï¼š

```python
import faiss
import torch

all_vecs = torch.load('all_h_change.pt').numpy()  # [N, D]
index = faiss.IndexFlatL2(all_vecs.shape[1])
index.add(all_vecs)
```

#### â‘¡ æŸ¥è¯¢æœ€ç›¸è¿‘çš„æ ·æœ¬ï¼ˆk è¿‘é‚»ï¼‰

```python
query = h_change.cpu().numpy().reshape(1, -1)
D, I = index.search(query, k=3)  # å¾—åˆ° top-k ç›¸ä¼¼æ ·æœ¬ index
```

#### â‘¢ æ‹¼æ¥ Promptï¼ˆç»“æ„å»ºè®®ï¼‰ï¼š

```text
Context:
  A: ...
  B: ...
  ...

Retrieved Examples:
Example 1:
Stimulus: Bè¡¨ç¤ºè‡ªå·±ä¸æ‡‚è£…ä¿®
Appraisal: Aè®¤ä¸ºBå·¥ä½œæ€åº¦ä¸å¥½
Response: Aæ„Ÿåˆ°æ„¤æ€’

Now explain the emotional change in this dialogue:
```

#### âœ… æ„é€ ä»£ç æ¨¡å—ï¼ˆå‡½æ•°æ¥å£ï¼‰

```python
def build_prompt(dialogue_context, retrieved_rationales, hint=None):
    prompt = "Context:\n" + dialogue_context + "\n\n"
    prompt += "Retrieved Examples:\n"
    for i, r in enumerate(retrieved_rationales):
        prompt += f"Example {i+1}:\n"
        prompt += f"Stimulus: {r['stimulus']['textual'] or ''}\n"
        prompt += f"Appraisal: {r['appraisal']}\n"
        prompt += f"Response: {r['response']}\n\n"
    prompt += "Now explain the emotional change in this dialogue:\n"
    return prompt
```

---

## ğŸ”„ ä¸¤è€…ååŒä½¿ç”¨çš„æ–¹å¼ï¼š

| é˜¶æ®µ  | åšæ³•                                                         |
| --- | ---------------------------------------------------------- |
| è®­ç»ƒæ—¶ | è¾“å…¥ promptï¼ˆå«ä¸Šä¸‹æ–‡ + æ£€ç´¢æ ·ä¾‹ï¼‰ï¼Œå¹¶åŒæ—¶ç»™æ¨¡å‹ä¸€ä¸ª cross-attn æ¥å£æ¥æ”¶ `h_change` |
| æ¨ç†æ—¶ | å¯ä»¥ä»…ç”¨ `h_change` æ£€ç´¢å‡ºç¤ºä¾‹æ‹¼ promptï¼Œä¹Ÿå¯ä¸€å¹¶é€å…¥æ¨¡å‹è¾…åŠ©ç”Ÿæˆ                 |

è¿™æ ·å°±å®ç°äº†ï¼š

* âœ¨ æ˜¾å¼æŒ‡ä»¤åŒ–å¼•å¯¼ï¼ˆé€šè¿‡æ£€ç´¢æ ·æœ¬ï¼‰
* âœ¨ éšå¼è¯­ä¹‰æ§åˆ¶ï¼ˆé€šè¿‡ cross-attn èåˆå›¾è¡¨ç¤ºï¼‰

---

## âœ… æ¥ä¸‹æ¥æˆ‘å¯ä»¥å¸®ä½ ï¼š

1. âœ³ï¸ å®ç° CrossAttention æ’ä»¶å¹¶é›†æˆ HuggingFace LLMï¼›
2. âœ³ï¸ æ„é€ æ£€ç´¢åº“ï¼ˆå»ºç«‹è®­ç»ƒé›† h\_change â†’ rationale çš„ FAISS ç´¢å¼•ï¼‰ï¼›
3. âœ³ï¸ å†™å¥½ `build_prompt()` æ‹¼æ¥å‡½æ•°ï¼›
4. âœ³ï¸ æ•´åˆè®­ç»ƒæ•°æ® â†’ prompt â†’ LLM + LoRA è®­ç»ƒå™¨ã€‚

è¯·å‘Šè¯‰æˆ‘ä½ æƒ³**å…ˆåšå“ªä¸€æ­¥**ï¼Ÿä¾‹å¦‚ï¼š

* âœ… â€œå…ˆæ­å¥½ FAISS æ£€ç´¢åº“â€
* âœ… â€œå…ˆå†™ CrossAttention å±‚é›†æˆ HuggingFace æ¨¡å‹â€
* âœ… â€œå…ˆæ„é€  Prompt ç¤ºä¾‹ç”¨äºç”Ÿæˆè°ƒè¯•â€

ä½ å†³å®šï¼Œæˆ‘ç»§ç»­æ¨è¿›ï¼
