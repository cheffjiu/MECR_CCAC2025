## 一、赛题解读与难点分析

本次 MECR_CCAC2025 赛题目标是：

> **在双人对话的多模态场景中，定位情感变化发生的片段，并为该变化生成一段包含「文本刺激；视觉刺激。认知评价。情感反应。」四部分的自然语言解释（rationale）。**

### 难点分析

1. **多模态信息融合**

   * 文本与视频帧在内容和时序上不完全对齐，需要设计机制捕捉两者共同引发的情感变化。
2. **情感变化定位**

   * 对话中并非所有情感波动都值得解释，需先确定“变化”片段再生成原因。
3. **因果推理与生成**

   * 生成的解释要具备因果链条：哪个刺激→如何评价→如何反应，要符合人类认知。
4. **少量样本与多样化输出**

   * 数据集只有 \~4 000 条标注，且同一类型的变化解释可能措辞千差万别，生成质量容易过拟合或跑题。

---

## 二、数据概览与数据流

### 赛题提供的数据

* **train.json**（4 147 条样本）和 **val.json**

  * 每条样本包含：

    * `utts`：一系列 utterance，每个含说话人、文本和 emotion 标签
    * `video_path`：对应对话视频片段
    * `target_change_utt_ids`：情感变化起止 utterance 的索引
    * `rationale`：训练时目标解释文本（四部分拼接）
* **memoconv\_convs/**

  * 按 `video_path` 组织的原始视频文件，可截帧或提取音频
* **readme.pdf / README.md**

  * 字段定义、评测方式、提交格式等说明

### 数据流全貌

1. **原始数据**

   * JSON + 视频文件
2. **预处理与特征提取**

   * 文本 → BERT embedding
   * 视频帧 → CLIP/Faster‑R-CNN embedding
3. **检索示例准备**

   * 从所有训练 `rationale`、“文本刺激”、“视觉刺激”中分别构建向量库
4. **图构建与编码**

   * 对话 utterance 构图，生成 “变化” 超级节点 embedding
5. **Prompt 拼接**

   * 将对话文本、变化向量、Top‑K 检索示例合并为多模态 Prompt
6. **LoRA 微调生成**

   * 在 GPT‑4V／Flamingo 上微调，生成最终解释文本

最终输出为每条测试样本的一条 rationale，自包含四个部分，可直接提交。

---

## 三、详细实施步骤

下面按照四个阶段，依次说明 **实施步骤**、**所用模型/技术**、**数据流（输入→处理→输出）** 及 **原因**，并为每一步给出示例。

---

### 阶段 1：数据准备与检索库构建

1. **解析原始 JSON**

   * **脚本**：`prepare_data.py`
   * **输入**：

     ```json
     {
       "id": "sample_001",
       "utts": [
         {"speaker":"A","utterance":"你好！","emotion":["neutral"]},
         {"speaker":"B","utterance":"我刚买了个红包。","emotion":["happy"]},
         …
       ],
       "target_change_utt_ids":[1,2],
       "rationale":"A 递给 B 红包；画面中红包图案。B 感到意外和欣喜。B 开心地笑了。"
     }
     ```
   * **处理**：清洗字段、统一 utterance 索引、输出标准化字典列表
   * **输出**：`data/parsed/sample_001.json`，同上述结构
   * **为什么**：统一后续模块调用接口，隔离原始多种格式

2. **文本特征提取**

   * **模型**：BERT‑Base
   * **输入**：`["你好！","我刚买了个红包。",…]`
   * **处理**：每条文本取 `[CLS]` 向量，拼成 `T ∈ ℝⁿˣ768`
   * **输出**：`features/text/sample_001.npy`，形状 `(n,768)`
   * **示例**：`T[1] = [0.12, -0.03, …]`
   * **为什么**：将离散文本转为稠密向量，便于后续相似度检索与图节点表示

3. **视觉特征提取**

   * **模型**：CLIP‑ViT
   * **输入**：目标变化前后各 3 帧图像路径
   * **处理**：逐帧提取 768‑D 全局特征，拼成 `V ∈ ℝᵐˣ768`
   * **输出**：`features/vision/sample_001.npy`，形状 `(m,768)`
   * **为什么**：捕捉视觉场景语义，与文本同维度方便融合

4. **检索库构建**

   * **对象**：全训练集的

     * 文本刺激（`;` 分割 rationale 首段）
     * 视觉刺激（第二段）
     * 完整 rationale
   * **技术**：FAISS IndexFlatIP
   * **输出**：三个索引文件

     * `indices/text_stim.idx`
     * `indices/vis_stim.idx`
     * `indices/full_rationale.idx`
   * **为什么**：后续生成时，用近似示例引导丰富输出，减少跑题

---

### 阶段 2：情感演化图构建与编码

1. **图结构生成**

   * **脚本**：`build_graph.py`
   * **输入**：`sample_001` 的 `T`、`V` 向量
   * **处理**：

     * 每条 utterance 构造节点，属性 `[t_i; v_i] ∈ ℝ¹⁵³⁶`
     * 新增超级节点 S，连边到起讫 utterance（如节点 1、2）
     * 添加顺序边与同说话人全连边
   * **输出**：`graphs/sample_001.pt`（PyG Data 对象）
   * **为什么**：用图结构显式表示对话时序与说话人关系，并聚焦变化段

2. **图编码**

   * **模型**：Temporal Graph Network
   * **输入**：`graphs/sample_001.pt`
   * **处理**：多层时序消息传递，迭代更新节点向量
   * **输出**：

     * `h_change ∈ ℝ⁷⁶⁸`（S 节点最终表示）
     * `h_nodes ∈ ℝⁿˣ⁷⁶⁸`（各 utterance 新表示）
   * **为什么**：提取跨句、跨说话人的深层情感演化特征，为生成提供变化核心向量

---

### 阶段 3：Prompt 组装 & LoRA 微调

1. **Top‑K 检索示例**

   * **代码**：

     ```python
     te_ids, _ = faiss_text.search(mean(T,axis=0), k=3)
     ve_ids, _ = faiss_vision.search(mean(V,axis=0), k=3)
     re_ids, _ = faiss_rationale.search(pool(target_rationale), k=3)
     ```
   * **输出**：文本示例列表 `["A 递红包…",…]`，视觉示例 `["画面中…",…]`，rationale 示例
   * **为什么**：以类似案例为生成提供上下文和表达模板

2. **Prompt 拼接**

   * **格式示例**：

     ```
     Context:
     A: 你好！
     B: 我刚买了个红包。
     ChangeVec: <tensor([0.12, …, -0.08])>
     RetrievedText: A 送出礼物；A 举起蛋糕；A 握住对方手
     RetrievedVis: 红包特写；蛋糕蜡烛；握手场景
     RetrievedRationale: A 表示祝福；B 惊喜；B 感动
     Generate rationale:
     ```
   * **为什么**：把所有必要信息以人类可读的形式呈给模型，同时嵌入变化向量和示例

3. **LoRA 微调**

   * **基模型**：GPT‑4V（HuggingFace 接口）
   * **技术**：PEFT LoRA，`r=8, α=16`
   * **损失**：

     * 交叉熵损失 $\mathcal{L}_{CE}$
     * 图-文本对齐对比损失 $\mathcal{L}_{contra}$，权重 λ=0.1
     * 总损失 $\mathcal{L} = \mathcal{L}_{CE} + 0.1\,\mathcal{L}_{contra}$
   * **输出**：微调后 LoRA 参数 `checkpoints/lora.pt`
   * **为什么**：用小量可训练参数快速适应新任务，兼顾生成质量与算力成本

---

### 阶段 4：推理与部署

1. **推理脚本**

   * 重复数据预处理、检索、图编码和 Prompt 拼接流程
   * 调用微调后的 LoRA 模型生成
   * **输入示例**：`sample_999` 原始 JSON + 视频帧
   * **输出示例**：

     ```
     A 递给 B 红包；镜头拉近红包。B 对这种惊喜感到愉悦和感激。B 微笑并道谢。
     ```
2. **评测与上线**

   * 用官方脚本计算 METEOR 与 BERTScore 排名和
   * 将生成服务化（FastAPI + TorchServe）

---

通过上述四个阶段的 **脚本、模型、数据流与示例**，完整覆盖从原始数据到高质量多模态情感变化解释的端到端实现。每一步都紧扣“为什么要做”与“达到什么目标”，确保方案可复现、可优化、可落地。祝项目成功！
